{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51dc07c0f7c94307a9cecf2ef5040d2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_682dbf62ef5346fdb93733a3e2dd8c69",
              "IPY_MODEL_b9d99f0b10e04533a9efce55eaa004c8",
              "IPY_MODEL_0b91ba0140414607802f2ce4f1892588"
            ],
            "layout": "IPY_MODEL_8c8c3880af3e486198e72b7a4d45f802"
          }
        },
        "682dbf62ef5346fdb93733a3e2dd8c69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87ddf418b2b74705af737c4c4b42c734",
            "placeholder": "​",
            "style": "IPY_MODEL_84f9a3eb3fe2419491719416e6fb640e",
            "value": "Map: 100%"
          }
        },
        "b9d99f0b10e04533a9efce55eaa004c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc9cfb643e1e4219b7696b782a4b1e9d",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb41ede830d948a98c6e445c046b11d1",
            "value": 25000
          }
        },
        "0b91ba0140414607802f2ce4f1892588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5417f59582d54899a463f7eb6de385fe",
            "placeholder": "​",
            "style": "IPY_MODEL_656a0d90e2334cedb6dd857f0213051e",
            "value": " 25000/25000 [00:41&lt;00:00, 613.85 examples/s]"
          }
        },
        "8c8c3880af3e486198e72b7a4d45f802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87ddf418b2b74705af737c4c4b42c734": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84f9a3eb3fe2419491719416e6fb640e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc9cfb643e1e4219b7696b782a4b1e9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb41ede830d948a98c6e445c046b11d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5417f59582d54899a463f7eb6de385fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "656a0d90e2334cedb6dd857f0213051e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puGHrjT00fPJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxEbluW80fPJ",
        "outputId": "85fab4c2-72a4-45da-f7bf-aea03043ef2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpVrQYKF0fPK",
        "outputId": "8355d92a-577a-4b85-d4d1-79c583bde2c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 25,000\n",
            "Test: 25,000\n",
            "\n",
            "Example: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ev...\n",
            "Label: 0\n"
          ]
        }
      ],
      "source": [
        "# load IMDB sentiment data\n",
        "dataset = load_dataset('imdb')\n",
        "\n",
        "print(f\"Train: {len(dataset['train']):,}\")\n",
        "print(f\"Test: {len(dataset['test']):,}\")\n",
        "print(f\"\\nExample: {dataset['train'][0]['text'][:200]}...\")\n",
        "print(f\"Label: {dataset['train'][0]['label']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "51dc07c0f7c94307a9cecf2ef5040d2a",
            "682dbf62ef5346fdb93733a3e2dd8c69",
            "b9d99f0b10e04533a9efce55eaa004c8",
            "0b91ba0140414607802f2ce4f1892588",
            "8c8c3880af3e486198e72b7a4d45f802",
            "87ddf418b2b74705af737c4c4b42c734",
            "84f9a3eb3fe2419491719416e6fb640e",
            "dc9cfb643e1e4219b7696b782a4b1e9d",
            "eb41ede830d948a98c6e445c046b11d1",
            "5417f59582d54899a463f7eb6de385fe",
            "656a0d90e2334cedb6dd857f0213051e"
          ]
        },
        "id": "kq-8yE0O0fPK",
        "outputId": "15ed71b7-dbf9-44a0-9d7e-29052322e219"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51dc07c0f7c94307a9cecf2ef5040d2a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# tokenize with BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "tokenized = dataset.map(tokenize, batched=True)\n",
        "tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkdNCv7d0fPK",
        "outputId": "d3db6a59-dd63-410f-e428-450827578afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT params: 109,482,240\n",
            "Head params: 49,281\n"
          ]
        }
      ],
      "source": [
        "# custom classification head - sits on top of frozen BERT\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "# load pretrained BERT\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "embedding_dim = bert.config.hidden_size\n",
        "\n",
        "head = ClassificationHead(embedding_dim)\n",
        "\n",
        "print(f\"BERT params: {sum(p.numel() for p in bert.parameters()):,}\")\n",
        "print(f\"Head params: {sum(p.numel() for p in head.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR4R9c8q0fPK"
      },
      "outputs": [],
      "source": [
        "# freeze BERT - only train the head\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "bert.to(device)\n",
        "head.to(device)\n",
        "\n",
        "# demo subset - production would use full 25k samples and more epochs\n",
        "train_data = tokenized['train'].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "def collate(batch):\n",
        "    return {\n",
        "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
        "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
        "        'labels': torch.tensor([b['label'] for b in batch], dtype=torch.float)\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True, collate_fn=collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqA0QoBk0fPK",
        "outputId": "d9958971-16ef-43d2-f834-78111877e13e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.6869\n",
            "Epoch 2/10, Loss: 0.6658\n",
            "Epoch 3/10, Loss: 0.6630\n",
            "Epoch 4/10, Loss: 0.6453\n",
            "Epoch 5/10, Loss: 0.6360\n",
            "Epoch 6/10, Loss: 0.6293\n",
            "Epoch 7/10, Loss: 0.6197\n",
            "Epoch 8/10, Loss: 0.6207\n",
            "Epoch 9/10, Loss: 0.6022\n",
            "Epoch 10/10, Loss: 0.5994\n"
          ]
        }
      ],
      "source": [
        "# training setup\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(head.parameters(), lr=2e-4)  # higher lr since BERT is frozen\n",
        "\n",
        "epochs = 10  # more passes needed for head to learn\n",
        "head.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].unsqueeze(1).to(device)\n",
        "\n",
        "        # get BERT embeddings without updating BERT\n",
        "        with torch.no_grad():\n",
        "            outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            pooled = outputs.pooler_output\n",
        "\n",
        "        # train only the head\n",
        "        logits = head(pooled)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m8zOn3F0fPL",
        "outputId": "43d0cd94-dd41-4ab1-c9e2-348cae7fc2bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "--------------------------------------------------\n",
            "This movie was amazing, loved every minu...\n",
            "  -> negative (39.32%)\n",
            "\n",
            "Terrible film, waste of time....\n",
            "  -> negative (35.59%)\n",
            "\n",
            "It was okay, nothing special....\n",
            "  -> negative (32.56%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# test predictions\n",
        "def predict(text):\n",
        "    encoded = tokenizer(text, return_tensors='pt', truncation=True, max_length=128).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = bert(**encoded)\n",
        "        pooled = outputs.pooler_output\n",
        "        logit = head(pooled)\n",
        "        prob = torch.sigmoid(logit).item()\n",
        "\n",
        "    return 'positive' if prob > 0.5 else 'negative', prob\n",
        "\n",
        "test_texts = [\n",
        "    \"This movie was amazing, loved every minute!\",\n",
        "    \"Terrible film, waste of time.\",\n",
        "    \"It was okay, nothing special.\"\n",
        "]\n",
        "\n",
        "print(\"Predictions:\")\n",
        "print(\"-\"*50)\n",
        "for text in test_texts:\n",
        "    label, prob = predict(text)\n",
        "    print(f\"{text[:40]}...\")\n",
        "    print(f\"  -> {label} ({prob:.2%})\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VHQjuoD0fPL",
        "outputId": "28fcb4f9-d78d-4e09-e95a-4fac562bdbc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 62.60%\n"
          ]
        }
      ],
      "source": [
        "# evaluation on test set\n",
        "test_data = tokenized['test'].shuffle(seed=42).select(range(500))\n",
        "test_loader = DataLoader(test_data, batch_size=32, collate_fn=collate)\n",
        "\n",
        "head.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = head(outputs.pooler_output)\n",
        "        preds = (torch.sigmoid(logits) > 0.5).squeeze().float()\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {correct/total:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHgHfehQ0fPL",
        "outputId": "5f31bc74-eb16-4a1b-893b-a96429bba916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SUMMARY\n",
            "============================================================\n",
            "\n",
            "Approach: Frozen BERT backbone + trainable classification head\n",
            "\n",
            "Why this works:\n",
            "  - BERT already encodes semantic understanding from pretraining\n",
            "  - We only need to train a small head for our specific task\n",
            "  - Much faster than fine-tuning all 110M BERT parameters\n",
            "\n",
            "Why transformers here but not for life expectancy:\n",
            "  - Text requires learned representations - cant use raw words\n",
            "  - Life expectancy data is already numerical - simpler models work\n",
            "  - Using 110M params for 7 features would be massive overkill\n",
            "\n",
            "Tradeoff: Transformers powerful but uninterpretable\n",
            "  - Cant explain why a review is classified positive/negative\n",
            "  - For text theres no real alternative, for tabular there is\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nApproach: Frozen BERT backbone + trainable classification head\")\n",
        "print(f\"\\nWhy this works:\")\n",
        "print(f\"  - BERT already encodes semantic understanding from pretraining\")\n",
        "print(f\"  - We only need to train a small head for our specific task\")\n",
        "print(f\"  - Much faster than fine-tuning all 110M BERT parameters\")\n",
        "print(f\"\\nWhy transformers here but not for life expectancy:\")\n",
        "print(f\"  - Text requires learned representations - cant use raw words\")\n",
        "print(f\"  - Life expectancy data is already numerical - simpler models work\")\n",
        "print(f\"  - Using 110M params for 7 features would be massive overkill\")\n",
        "print(f\"\\nTradeoff: Transformers powerful but uninterpretable\")\n",
        "print(f\"  - Cant explain why a review is classified positive/negative\")\n",
        "print(f\"  - For text theres no real alternative, for tabular there is\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frozen backbone made more sense here than fine-tuning everything since we only have 1000 training samples and BERT already understands language structure. The key point for my assignment is that transformers are necessary for text because there's no sensible way to feed raw words into a Random Forest, but that same reasoning doesn't apply to tabular data where the features are already numeric. Using 110 million parameters on seven columns would be solving a problem that doesn't exist."
      ],
      "metadata": {
        "id": "EoUkMtiA0fPL"
      }
    }
  ]
}